# -*- coding: utf-8 -*-
"""Allstate_Claims_Severity_Modeling_1115_Top_10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ln8xepFBSUgOsjnzdoUbFO_fPC9KN4lb

<h1> Referrence from Kaggle Competition to be in top 10% </h1>

DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed. You can construct DMatrix from multiple different sources of data.

**Referrence**

1. https://ml.dask.org/modules/generated/dask_ml.xgboost.XGBRegressor.html
2. https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f
3. https://stackoverflow.com/questions/61471765/xgboost-what-data-to-use-in-the-watchlist
4. https://www.kaggle.com/mtinti/xgb-1110-from-vladimir-iglovikov-and-tilii7
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/My Drive/Applied AI Course/Assignments/23. Self Case Study 1

"""<h1> Hyper Parameter Tuning </h1>"""

gridsearch_params = [
    (max_depth,subsample)
    for max_depth in range(4,18,4)
	for subsample in [n/10 for n in range(2,12,2)]
]
params = {}

"""
Reading Train and Test data
"""
Train_Data = pd.read_csv('train.csv')
Test_Data = pd.read_csv('test.csv')
Submission_Id = Test_Data['id']

r,c = Train_Data.shape   # r -> rows and c  --> columns # 188318 and 130

y = np.log(Train_Data['loss'] + 200)

Train_Data.drop(['id','loss'], axis=1, inplace=True)
Test_Data.drop(['id'], axis=1, inplace=True)

Train_Test = pd.concat((Train_Data, Test_Data)).reset_index(drop=True)

def evalerror(preds, dtrain):
    labels = dtrain.get_label()
    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))

if __name__ == '__main__':
  cat_feature = [n for n in Train_Data.columns if n.startswith('cat')]
  for column in cat_feature:
    if Train_Data[column].nunique() != Test_Data[column].nunique():
      Unique_classes_Train = set(Train_Data[column].unique())
      Unique_classes_Test = set(Test_Data[column].unique())
      missing_train = Unique_classes_Train.difference(Unique_classes_Test)           # set_A.difference(set_B) for (A - B)
      missing_test =  Unique_classes_Test.difference(Unique_classes_Train)

      All_misisng = missing_train.union(missing_test)

      # Replace all misisng categories with a common category instead of removing. 
      def missing_common(x):
        if x in All_misisng:
          return np.nan
        return x
      
      Train_Test[column] = Train_Test[column].apply(lambda x: missing_common(x), 1)   # Axis 1 :: columns
    
    Train_Test[column] = pd.factorize(Train_Test[column].values, sort=True)[0]

  Train_Data_final = Train_Test.iloc[:r, :]
  Test_Data_final = Train_Test.iloc[r:, :]  

  min_mae = float("Inf")
  best_params = None 

  xgtrain = xgb.DMatrix(Train_Data_final, label=y)

  for max_depth, subsample in gridsearch_params:
    print("CV with max_depth={}, subsample={}".format(max_depth, subsample))

    params['max_depth'] = max_depth
    params['subsample'] = subsample

    cv_results = xgb.cv(
        params,
        xgtrain,
        1000,
        seed=42,
        nfold=3,
        metrics={'mae'},
        early_stopping_rounds=10
    )

    mean_mae = cv_results['test-mae-mean'].min()
    boost_rounds = cv_results['test-mae-mean'].argmin()
    print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
    if mean_mae < min_mae:
        min_mae = mean_mae
        best_params = (max_depth, subsample)

    print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))

gridsearch_params = [
    (alpha,gamma)
    for alpha in [1/n for n in range(1,3)]
	for gamma in [1/n for n in range(1,3)]
]

params = {}

for alpha, gamma in gridsearch_params:
  print("CV with alpha={}, gamma={}".format(alpha, gamma))
  params['max_depth'] = 12
  params['subsample'] = 0.8
  params['alpha'] = alpha
  params['gamma'] = gamma
  
  cv_results = xgb.cv(
        params,
        xgtrain,
        1000,
        seed=42,
        nfold=3,
        metrics={'mae'},
        early_stopping_rounds=10
    )
  mean_mae = cv_results['test-mae-mean'].min()
  boost_rounds = cv_results['test-mae-mean'].argmin()
  print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
  if mean_mae < min_mae:
    min_mae = mean_mae
    best_params = (alpha, gamma)
    
  print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))

gridsearch_params = [
    (colsample_bytree,eta)
    for colsample_bytree in [1/n for n in range(1,3)]
	  for eta in [1/(10**n) for n in range(1,4)]
]

params = {}

for colsample_bytree, eta in gridsearch_params:
  print("CV with colsample_bytree={}, eta={}".format(colsample_bytree, eta))
  params['max_depth'] = 12
  params['subsample'] = 0.8
  params['alpha'] = 1
  params['gamma'] = 1
  params['colsample_bytree'] = colsample_bytree
  params['eta'] = eta
  
  cv_results = xgb.cv(
        params,
        xgtrain,
        1000,
        seed=42,
        nfold=3,
        metrics={'mae'},
        early_stopping_rounds=10
    )
  mean_mae = cv_results['test-mae-mean'].min()
  boost_rounds = cv_results['test-mae-mean'].argmin()
  print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
  if mean_mae < min_mae:
    min_mae = mean_mae
    best_params = (colsample_bytree, eta)
    
  print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))

"""<h1> Final Code </h1>"""

"""
Reading Train and Test data
"""
Train_Data = pd.read_csv('train.csv')
Test_Data = pd.read_csv('test.csv')
Submission_Id = Test_Data['id']

r,c = Train_Data.shape   # r -> rows and c  --> columns # 188318 and 130

y = np.log(Train_Data['loss'] + 200)

Train_Data.drop(['id','loss'], axis=1, inplace=True)
Test_Data.drop(['id'], axis=1, inplace=True)

Train_Test = pd.concat((Train_Data, Test_Data)).reset_index(drop=True)

def evalerror(preds, dtrain):
    labels = dtrain.get_label()
    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))

if __name__ == '__main__':
  cat_feature = [n for n in Train_Data.columns if n.startswith('cat')]
  for column in cat_feature:
    if Train_Data[column].nunique() != Test_Data[column].nunique():
      Unique_classes_Train = set(Train_Data[column].unique())
      Unique_classes_Test = set(Test_Data[column].unique())
      missing_train = Unique_classes_Train.difference(Unique_classes_Test)           # set_A.difference(set_B) for (A - B)
      missing_test =  Unique_classes_Test.difference(Unique_classes_Train)

      All_misisng = missing_train.union(missing_test)

      # Replace all misisng categories with a common category instead of removing. 
      def missing_common(x):
        if x in All_misisng:
          return np.nan
        return x
      
      Train_Test[column] = Train_Test[column].apply(lambda x: missing_common(x), 1)   # Axis 1 :: columns
    
    Train_Test[column] = pd.factorize(Train_Test[column].values, sort=True)[0]

  Train_Data_final = Train_Test.iloc[:r, :]
  Test_Data_final = Train_Test.iloc[r:, :]

  kf = KFold(n_splits=3)
  prediction = np.zeros(Submission_Id.shape)

  for train_index, test_index in kf.split(Train_Data_final):
    X_train, X_test = Train_Data_final.iloc[train_index], Train_Data_final.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    params = {
            'min_child_weight': 1,
            'eta': 0.01,
            'colsample_bytree': 0.5,
            'max_depth': 12,
            'subsample': 0.8,
            'alpha': 1,
            'gamma': 1,
            'silent': 1,
            'verbose_eval': True,
            'seed': 2020,
        }
      
    xgtrain = xgb.DMatrix(X_train, label=y_train)
    xgtest  = xgb.DMatrix(X_test, label=y_test)
    xgtest_final = xgb.DMatrix(Test_Data_final)

    watchlist = [(xgtrain, 'train'), (xgtest, 'eval')]

    model = xgb.train(params, xgtrain, 2000, watchlist, feval=evalerror, early_stopping_rounds=100) 
    prediction += np.exp(model.predict(xgtest_final)) - 200
  
  prediction = prediction/3

import pickle
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))